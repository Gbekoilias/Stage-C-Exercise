{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMiHfbYa8PkXqMvzSWUZS2k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KSs11In3t_sZ","executionInfo":{"status":"ok","timestamp":1696608137262,"user_tz":-60,"elapsed":552,"user":{"displayName":"Shittu-Gbeko Ilias Olakunle","userId":"02327692346623940216"}},"outputId":"c8b584ac-058e-4e2d-b750-bb1d229f7207"},"outputs":[{"output_type":"stream","name":"stdout","text":["No table found on the webpage. Check the HTML structure.\n"]}],"source":["\n","import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# Replace with the URL of the webpage containing the flood dataset\n","url = 'https://floodlist.com/data-api'  # Update this with the actual URL\n","\n","try:\n","    # Send a GET request\n","    response = requests.get(url)\n","\n","    # Check if the request was successful\n","    response.raise_for_status()\n","\n","    # Parse the webpage\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    # Assuming the data is in a table, find the table element\n","    table = soup.find('table', class_='flood-data-table')\n","\n","\n","    if table:\n","        # Create a CSV file to store the data\n","        with open('flood_dataset.csv', 'w', newline='') as csv_file:\n","            csv_writer = csv.writer(csv_file)\n","\n","            # Write headers (assuming headers are in the first row of the table)\n","            headers = [header.text for header in table.find_all('th')]\n","            csv_writer.writerow(headers)\n","\n","            # Extract and write data to the CSV file\n","            for row in table.find_all('tr')[1:]:  # Skip the header row\n","                data = [cell.text for cell in row.find_all('td')]\n","                csv_writer.writerow(data)\n","\n","        print('Data has been successfully scraped and saved to \"flood_dataset.csv\".')\n","    else:\n","        print('No table found on the webpage. Check the HTML structure.')\n","\n","except requests.exceptions.RequestException as e:\n","    print(f'Error: {e}')\n","\n","except Exception as ex:\n","    print(f'An error occurred: {ex}')\n","\n"]},{"cell_type":"code","source":["url = 'https://www.jbarisk.com/products-services/event-response/nigeria-flooding/'\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# Replace with the URL of the webpage containing the flood dataset\n","  # Update this with the actual URL\n","\n","try:\n","    # Send a GET request\n","    response = requests.get(url)\n","\n","    # Check if the request was successful\n","    response.raise_for_status()\n","\n","    # Parse the webpage\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    # Assuming the data is in a table, find the table element\n","    table = soup.find('table', class_='flood-data-table')\n","\n","\n","    if table:\n","        # Create a CSV file to store the data\n","        with open('flood_dataset.csv', 'w', newline='') as csv_file:\n","            csv_writer = csv.writer(csv_file)\n","\n","            # Write headers (assuming headers are in the first row of the table)\n","            headers = [header.text for header in table.find_all('th')]\n","            csv_writer.writerow(headers)\n","\n","            # Extract and write data to the CSV file\n","            for row in table.find_all('tr')[1:]:  # Skip the header row\n","                data = [cell.text for cell in row.find_all('td')]\n","                csv_writer.writerow(data)\n","\n","        print('Data has been successfully scraped and saved to \"flood_dataset.csv\".')\n","    else:\n","        print('No table found on the webpage. Check the HTML structure.')\n","\n","except requests.exceptions.RequestException as e:\n","    print(f'Error: {e}')\n","\n","except Exception as ex:\n","    print(f'An error occurred: {ex}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pO2epbCmxprd","executionInfo":{"status":"ok","timestamp":1696609013596,"user_tz":-60,"elapsed":565,"user":{"displayName":"Shittu-Gbeko Ilias Olakunle","userId":"02327692346623940216"}},"outputId":"cd17c16c-7acb-4b49-af8e-85dc0afd5cc1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["No table found on the webpage. Check the HTML structure.\n"]}]},{"cell_type":"code","source":["url = 'https://data.humdata.org/dataset/nigeria-nema-flood-affected-geographical-areasnorth-east-nigeria-flood-affected-geographical-areas'\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# Replace with the URL of the webpage containing the flood dataset\n","  # Update this with the actual URL\n","\n","try:\n","    # Send a GET request\n","    response = requests.get(url)\n","\n","    # Check if the request was successful\n","    response.raise_for_status()\n","\n","    # Parse the webpage\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    # Assuming the data is in a table, find the table element\n","    table = soup.find('table', class_='flood-data-table')\n","\n","\n","    if table:\n","        # Create a CSV file to store the data\n","        with open('flood_dataset.csv', 'w', newline='') as csv_file:\n","            csv_writer = csv.writer(csv_file)\n","\n","            # Write headers (assuming headers are in the first row of the table)\n","            headers = [header.text for header in table.find_all('th')]\n","            csv_writer.writerow(headers)\n","\n","            # Extract and write data to the CSV file\n","            for row in table.find_all('tr')[1:]:  # Skip the header row\n","                data = [cell.text for cell in row.find_all('td')]\n","                csv_writer.writerow(data)\n","\n","        print('Data has been successfully scraped and saved to \"flood_dataset.csv\".')\n","    else:\n","        print('No table found on the webpage. Check the HTML structure.')\n","\n","except requests.exceptions.RequestException as e:\n","    print(f'Error: {e}')\n","\n","except Exception as ex:\n","    print(f'An error occurred: {ex}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sBf8Ep6exwY5","executionInfo":{"status":"ok","timestamp":1696609333035,"user_tz":-60,"elapsed":1331,"user":{"displayName":"Shittu-Gbeko Ilias Olakunle","userId":"02327692346623940216"}},"outputId":"dd17d041-26e7-415f-ba3e-69d34aed394e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["No table found on the webpage. Check the HTML structure.\n"]}]},{"cell_type":"code","source":["import requests\n","import json\n","import csv\n","\n","# Specify the API endpoint URL\n","api_url = 'https://api.example.com/data'  # Replace with the actual API endpoint URL\n","\n","# Send a GET request to the API\n","response = requests.get(api_url)\n","\n","# Check if the request was successful (status code 200)\n","if response.status_code == 200:\n","    # Parse the JSON response\n","    data = response.json()\n","\n","    # Assuming the data is a list of dictionaries, you can extract and process it\n","    for item in data:\n","        # Process each item as needed\n","        item_data = item.get('data_field')  # Replace 'data_field' with the actual field name\n","\n","        # Store the data in a CSV file\n","        with open('api_data.csv', 'a', newline='') as csv_file:\n","            csv_writer = csv.writer(csv_file)\n","\n","            # Write the data to the CSV file\n","            csv_writer.writerow([item_data])\n","\n","    print('Data has been successfully retrieved and saved to \"api_data.csv\".')\n","else:\n","    print(f'Failed to retrieve data from the API. Status Code: {response.status_code}')\n","\n"],"metadata":{"id":"On6ly6c_zU2U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","import pandas as pd\n","\n","# URL of the website containing the CSV or Excel file links\n","website_url = 'https://data.humdata.org/dataset/nigeria-nema-flood-affected-geographical-areasnorth-east-nigeria-flood-affected-geographical-areas'  # Replace with the actual URL\n","\n","# Send a GET request to the website\n","response = requests.get(website_url)\n","\n","# Check if the request was successful\n","if response.status_code == 200:\n","    # Parse the HTML content of the webpage\n","    html_content = response.text\n","\n","    # Assuming the CSV or Excel file links are in anchor (a) tags, extract them\n","    links = []\n","\n","    # Parse the HTML content with BeautifulSoup or any other HTML parsing library if needed\n","    # For example, using BeautifulSoup:\n","    # from bs4 import BeautifulSoup\n","    # soup = BeautifulSoup(html_content, 'html.parser')\n","    # Find and extract the CSV or Excel file links from the soup object\n","\n","    # Iterate through the links and download the files\n","    for link in links:\n","        file_url = link['href']  # Assuming the links have 'href' attributes\n","        file_name = link.text  # Use the link text as the file name\n","\n","        # Check if the file is a CSV or Excel file (you can add more extensions if needed)\n","        if file_url.endswith(('.csv', '.xls', '.xlsx')):\n","            # Send a GET request to download the file\n","            file_response = requests.get(file_url)\n","\n","            if file_response.status_code == 200:\n","                # Save the file locally\n","                with open(file_name, 'wb') as file:\n","                    file.write(file_response.content)\n","                print(f'Successfully downloaded: {file_name}')\n","            else:\n","                print(f'Failed to download: {file_name}')\n","\n","else:\n","    print('Failed to retrieve the webpage.')\n","\n"],"metadata":{"id":"4fxQ95Wlz95o","executionInfo":{"status":"ok","timestamp":1696609694314,"user_tz":-60,"elapsed":1928,"user":{"displayName":"Shittu-Gbeko Ilias Olakunle","userId":"02327692346623940216"}}},"execution_count":5,"outputs":[]}]}